{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 07 - Extracting Information from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resource: https://www.nltk.org/book/ch07.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "  sentence = nltk.sent_tokenize(document)\n",
    "  tokens = [nltk.word_tokenize(sent) for sent in sentence]\n",
    "  tagged_tokens = [nltk.pos_tag(token) for token in tokens]\n",
    "  return tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The curious cat sat on the warm, sunny windowsill.\"\n",
    "tagged_sentences = ie_preprocess(sentence)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT curious/JJ cat/NN)\n",
      "  sat/VBD\n",
      "  on/IN\n",
      "  (NP the/DT warm/NN)\n",
      "  ,/,\n",
      "  (NP sunny/JJ windowsill/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "for tagged_sentence in tagged_sentences:\n",
    "  result = cp.parse(tagged_sentence)\n",
    "  print(result)\n",
    "  result.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "      {<NNP>+}                # chunk sequences of proper nouns\n",
    "\"\"\"\n",
    "sentences = \"The curious cat sat on John's beautiful garden in New York City.\"\n",
    "tagged_sentences = ie_preprocess(sentences)\n",
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT curious/JJ cat/NN)\n",
      "  sat/VBD\n",
      "  on/IN\n",
      "  (NP John/NNP)\n",
      "  's/POS\n",
      "  (NP beautiful/JJ garden/NN)\n",
      "  in/IN\n",
      "  (NP New/NNP York/NNP City/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "for tagged_sentence in tagged_sentences:\n",
    "  result = cp.parse(tagged_sentence)\n",
    "  print(result)\n",
    "  result.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CHUNK combined/VBN to/TO achieve/VB)\n",
      "(CHUNK continue/VB to/TO place/VB)\n",
      "(CHUNK serve/VB to/TO protect/VB)\n",
      "(CHUNK wanted/VBD to/TO wait/VB)\n",
      "(CHUNK allowed/VBN to/TO place/VB)\n",
      "(CHUNK expected/VBN to/TO become/VB)\n",
      "(CHUNK expected/VBN to/TO approve/VB)\n",
      "(CHUNK expected/VBN to/TO make/VB)\n",
      "(CHUNK intends/VBZ to/TO make/VB)\n",
      "(CHUNK seek/VB to/TO set/VB)\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser('CHUNK: {<V.*> <TO> <V.*>}')  # Chunk verb followed by \"to\" and another verb\n",
    "brown = nltk.corpus.brown\n",
    "i = 0  # Initialize a counter\n",
    "\n",
    "# Parse sentences in the Brown corpus\n",
    "for sent in brown.tagged_sents():\n",
    "    tree = cp.parse(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'CHUNK':  # Check if the subtree is a chunk\n",
    "            print(subtree)  # Print the chunk\n",
    "            i += 1\n",
    "            if i >= 10:  # Stop after 10 chunks\n",
    "                break\n",
    "    if i >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('NP', [('he', 'PRP')]), ('accepted', 'VBD'), Tree('NP', [('the', 'DT'), ('position', 'NN')]), ('of', 'IN'), Tree('NP', [('vice', 'NN'), ('chairman', 'NN')]), ('of', 'IN'), Tree('NP', [('Carlyle', 'NNP'), ('Group', 'NNP')]), (',', ','), Tree('NP', [('a', 'DT'), ('merchant', 'NN'), ('banking', 'NN'), ('concern', 'NN')]), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "he PRP B-NP\n",
    "accepted VBD B-VP\n",
    "the DT B-NP\n",
    "position NN I-NP\n",
    "of IN B-PP\n",
    "vice NN B-NP\n",
    "chairman NN I-NP\n",
    "of IN B-PP\n",
    "Carlyle NNP B-NP\n",
    "Group NNP I-NP\n",
    ", , O\n",
    "a DT B-NP\n",
    "merchant NN I-NP\n",
    "banking NN I-NP\n",
    "concern NN I-NP\n",
    ". . O\n",
    "'''\n",
    "nltk.chunk.conllstr2tree(text, chunk_types=['NP']).draw()\n",
    "print(nltk.chunk.conllstr2tree(text, chunk_types=['NP'])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT dollar/NN)\n",
      "  posted/VBD\n",
      "  (NP gains/NNS)\n",
      "  in/IN\n",
      "  (NP quiet/JJ trading/NN)\n",
      "  as/IN\n",
      "  (NP concerns/NNS)\n",
      "  about/IN\n",
      "  (NP equities/NNS)\n",
      "  abated/VBN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.conll2000.chunked_sents('train.txt', chunk_types=['NP'])[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  43.4%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "cp = nltk.RegexpParser(\"\")\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "print(cp.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%%\n",
      "    Precision:     70.6%%\n",
      "    Recall:        67.8%%\n",
      "    F-Measure:     69.2%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "  def __init__(self, train_sents):\n",
    "    train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                  for sent in train_sents]\n",
    "    self.tagger = nltk.UnigramTagger(train_data)\n",
    "\n",
    "  def parse(self, sentence):\n",
    "    pos_tags = [pos for (word,pos) in sentence]\n",
    "    tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "    chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "    conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                  in zip(sentence, chunktags)]\n",
    "    return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%%\n",
      "    Precision:     79.9%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     83.2%%\n"
     ]
    }
   ],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "unigram_chunker = UnigramChunker(train_sents)\n",
    "print(unigram_chunker.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 'B-NP'),\n",
       " (',', 'O'),\n",
       " ('DT', 'B-NP'),\n",
       " ('JJR', 'B-NP'),\n",
       " ('NNPS', 'I-NP'),\n",
       " ('PRP$', 'B-NP'),\n",
       " ('SYM', 'O'),\n",
       " ('VBG', 'O'),\n",
       " ('WP', 'B-NP')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postags = sorted(set(pos for sent in train_sents\n",
    "                      for (word,pos) in sent.leaves()))\n",
    "unigram_chunker.tagger.tag(postags)[:50:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "  def __init__(self, train_sents):\n",
    "    train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                  for sent in train_sents]\n",
    "    self.tagger = nltk.UnigramTagger(train_data)\n",
    "\n",
    "  def parse(self, sentence):\n",
    "    pos_tags = [pos for (word,pos) in sentence]\n",
    "    tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "    chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "    conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                  in zip(sentence, chunktags)]\n",
    "    return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%%\n",
      "    Precision:     79.9%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     83.2%%\n"
     ]
    }
   ],
   "source": [
    "bigram_checker = BigramChunker(train_sents)\n",
    "print(bigram_checker.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
    "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "  \"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),\n",
    "    (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP John/NNP)\n",
      "  thinks/VBZ\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
     ]
    }
   ],
   "source": [
    "sentence = [(\"John\", \"NNP\"), (\"thinks\", \"VBZ\"), (\"Mary\", \"NN\"),\n",
    "    (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"sit\", \"VB\"),\n",
    "    (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP John/NNP)\n",
      "  thinks/VBZ\n",
      "  (CLAUSE\n",
      "    (NP Mary/NN)\n",
      "    (VP\n",
      "      saw/VBD\n",
      "      (CLAUSE\n",
      "        (NP the/DT cat/NN)\n",
      "        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(grammar, loop=2)\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,224.0,168.0\" width=\"224px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"25%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Alice</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.5%\" y1=\"20px\" y2=\"48px\" /><svg width=\"75%\" x=\"25%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"38.0952%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">chased</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.0476%\" y1=\"20px\" y2=\"48px\" /><svg width=\"61.9048%\" x=\"38.0952%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"38.4615%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.2308%\" y1=\"20px\" y2=\"48px\" /><svg width=\"61.5385%\" x=\"38.4615%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">rabbit</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"69.2308%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"69.0476%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.5%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('NP', ['Alice']), Tree('VP', ['chased', Tree('NP', ['the', 'rabbit'])])])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree1 = nltk.Tree('NP', ['Alice'])\n",
    "tree2 = nltk.Tree('NP', ['the', 'rabbit'])\n",
    "tree3 = nltk.Tree('VP', ['chased', tree2])\n",
    "tree4 = nltk.Tree('S', [tree1, tree3])\n",
    "tree4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(VP chased (NP the rabbit))\n"
     ]
    }
   ],
   "source": [
    "print(tree4[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VP'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree4[1].label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice', 'chased', 'the', 'rabbit']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree4.leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rabbit'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree4[1][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Alice) (VP chased (NP the rabbit)))\n"
     ]
    }
   ],
   "source": [
    "print(tree4)\n",
    "tree4.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( S ( NP Alice ) ( VP chased ( NP the rabbit ) ) ) "
     ]
    }
   ],
   "source": [
    "# Define the traverse function\n",
    "def traverse(t):\n",
    "    try:\n",
    "        t.label()  # Check if the node is a subtree\n",
    "    except AttributeError:\n",
    "        print(t, end=\" \")  # Leaf node, print the content\n",
    "    else:\n",
    "        print('(', t.label(), end=\" \")  # Print the label of the subtree\n",
    "        for child in t:\n",
    "            traverse(child)  # Recursively visit the children\n",
    "        print(')', end=\" \")  # Close the current subtree\n",
    "\n",
    "traverse(tree4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "  for rel in nltk.sem.extract_rels('ORG', 'LOC', doc,\n",
    "                                corpus='ieer', pattern = IN):\n",
    "    print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAN(\"cornet_d'elzius\", 'buitenlandse_handel')\n",
      "VAN('johan_rottiers', 'kardinaal_van_roey_instituut')\n",
      "VAN('annie_lennox', 'eurythmics')\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2002\n",
    "vnv = \"\"\"\n",
    "(\n",
    "is/V|    # 3rd sing present and\n",
    "was/V|   # past forms of the verb zijn ('be')\n",
    "werd/V|  # and also present\n",
    "wordt/V  # past of worden ('become)\n",
    ")\n",
    ".*       # followed by anything\n",
    "van/Prep # followed by van ('of')\n",
    "\"\"\"\n",
    "VAN = re.compile(vnv, re.VERBOSE)\n",
    "for doc in conll2002.chunked_sents('ned.train'):\n",
    "    for rel in nltk.sem.extract_rels('PER', 'ORG', doc,\n",
    "                                  corpus='conll2002', pattern=VAN):\n",
    "      print(nltk.sem.clause(rel, relsym=\"VAN\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
